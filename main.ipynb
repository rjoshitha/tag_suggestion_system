! pip install scikit-multilearn
import numpy as np
import pandas as pd
# The questions
ENCODING = 'ISO-8859-1'

df_q = pd.read_csv("drive/My Drive/B2_NLP_Team_9/dataset/Questions.csv", encoding=ENCODING)
df_q.head()
# the tags

df_t = pd.read_csv("drive/My Drive/B2_NLP_Team_9/dataset/Tags.csv", encoding=ENCODING)
df_t.head()
df_t['Tag'] = df_t['Tag'].astype(str)

# group all tags given to same question into a single string
grouped_tags = df_t.groupby('Id')['Tag'].apply(lambda tags: ' '.join(tags))

grouped_tags.head()
# reset index for simplicity
grouped_tags.reset_index()

df_tags_final = pd.DataFrame({'Id': grouped_tags.index, 'Tags': grouped_tags.values})

df_tags_final.head()
# Drop unnecessary columns from questions
df_q.drop(columns=['OwnerUserId', 'CreationDate', 'ClosedDate'], inplace=True)
# Merge questions and tags into a single dataframe
df = df_q.merge(df_tags_final, on='Id')

del df_q
del df_t

df.head()
# remove questions with score lower than 5
df = df[df['Score'] > 5]

print(df.shape)
# split tags into list
df['Tags'] = df['Tags'].apply(lambda tags: tags.lower().split())

df.head()
# get all tags in the dataset
all_tags = []

for tags in df['Tags'].values:
    for tag in tags:
        all_tags.append(tag)
        
# print(all_tags)
import nltk

# create a frequency list of the tags
tag_freq = nltk.FreqDist(list(all_tags))

# get most common tags
tag_freq.most_common(25)
# get the most common 20 tags without the count
tag_features = list(map(lambda x: x[0], tag_freq.most_common(20)))

print(tag_features)
# Filter the tags from the dataset and remove all tags that does not belong to the tag_features
def keep_common(tags):
    
    filtered_tags = []
    
    # filter tags
    for tag in tags:
        if tag in tag_features:
            filtered_tags.append(tag)
    
    # return the filtered tag list
    return filtered_tags

# apply the function to filter in dataset
df['Tags'] = df['Tags'].apply(lambda tags: keep_common(tags))

# set the Tags column as None for those that do not have a most common tag
df['Tags'] = df['Tags'].apply(lambda tags: tags if len(tags) > 0 else None)

# Now we will drop all the columns that contain None in Tags column
df.dropna(subset=['Tags'], inplace=True)

df.shape
import re
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import ToktokTokenizer
from nltk.stem.snowball import SnowballStemmer

tokenizer = ToktokTokenizer()
stemmer = SnowballStemmer('english')
stop_words = set(stopwords.words('english'))

# Preprocess the text for vectorization
# - Remove HTML
# - Remove stopwords
# - Remove special characters
# - Convert to lowercase
# - Stemming

def remove_html(text):
    # Remove html and convert to lowercase
    return re.sub(r"\<[^\>]\>", "", text).lower()

def remove_stopwords(text):    
    # tokenize the text
    words = tokenizer.tokenize(text)
    
    filtered = [w for w in words if not w in stop_words]
    return ' '.join(map(str, filtered))

def remove_punc(text):
    #tokenize
    tokens = tokenizer.tokenize(text)
    
    # remove punctuations from each token
    tokens = list(map(lambda token: re.sub(r"[^A-Za-z0-9]+", " ", token).strip(), tokens))
    
    # remove empty strings from tokens
    tokens = list(filter(lambda token: token, tokens))
    
    return ' '.join(map(str, tokens))

def stem_text(text):
    #tokenize
    tokens = tokenizer.tokenize(text)
    
    # stem each token
    tokens = list(map(lambda token: stemmer.stem(token), tokens))
    
    return ' '.join(map(str, tokens))
    # drop Id and Score columns since we don't need them
df.drop(columns=['Id', 'Score'], inplace=True)

df.head()
# apply preprocessing to title and bodya
df['Title'] = df['Title'].apply(lambda x: remove_html(x))
df['Title'] = df['Title'].apply(lambda x: remove_stopwords(x))
df['Title'] = df['Title'].apply(lambda x: remove_punc(x))
df['Title'] = df['Title'].apply(lambda x: stem_text(x))

df.head()
# apply preprocessing to title and body
df['Body'] = df['Body'].apply(lambda x: remove_html(x))
df['Body'] = df['Body'].apply(lambda x: remove_stopwords(x))
df['Body'] = df['Body'].apply(lambda x: remove_punc(x))
df['Body'] = df['Body'].apply(lambda x: stem_text(x))

df.head()
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from scipy.sparse import hstack
X_title = df['Title']
X_body = df['Body']
y = df['Tags']

del df

# binarize our tags 
binarizer = MultiLabelBinarizer()
y_bin = binarizer.fit_transform(y)
# vectorize
vectorizer_title = TfidfVectorizer(
    analyzer = 'word', 
    strip_accents = None, 
    encoding = 'utf-8', 
    preprocessor=None, 
    max_features=10000)

vectorizer_body = TfidfVectorizer(
    analyzer = 'word', 
    strip_accents = None, 
    encoding = 'utf-8', 
    preprocessor=None, 
    max_features=10000)

X_title_vect = vectorizer_title.fit_transform(X_title)
X_body_vect = vectorizer_body.fit_transform(X_body)

X = hstack([X_title_vect, X_body_vect])
# train test split
X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size = 0.2, random_state = 0)

del X
def j_score(y_true, y_pred):
  jaccard = np.minimum(y_true, y_pred).sum(axis = 1)/np.maximum(y_true, y_pred).sum(axis = 1)
  return jaccard.mean()

def print_score(y_test, y_pred):
  # calculate Jacard score
  print('Jacard score: {}'.format(j_score(y_test, y_pred.toarray())))
  print('----')
  # # calculate Accuracy
  # print("Accuracy:", accuracy_score(y_test, y_pred))
  # print('----')
  # calculate recall
  print("Recall:", recall_score(y_true=y_test, y_pred=y_pred, average='weighted'))
  print('----')
  # calculate precision
  print("Precision: ", precision_score(y_true=y_test, y_pred=y_pred, average='weighted'))
  print('----')
  # calculate hamming loss
  print("Hamming Loss (%): ", hamming_loss(y_pred, y_test)*100)
  print('----')
  # calculate F1 score
  print("F1 Score: ", f1_score(y_pred, y_test, average='weighted'))
  print('----')
  # Develop the model
from sklearn.svm import LinearSVC
from skmultilearn.problem_transform import BinaryRelevance # gives better precision

svc = LinearSVC()
clf = BinaryRelevance(svc)

# fit training data
clf.fit(X_train, y_train)
# Metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, hamming_loss, f1_score

# make prediction
y_pred = clf.predict(X_test)

print_score(y_test, y_pred)
df_x = pd.concat([X_title, X_body], axis= 1)
X_train1, X_test1, y_train1, y_test1 = train_test_split(df_x, y, test_size = 0.2, random_state = 0)
df_x = pd.DataFrame(X_test1)
df_x.reset_index(drop=True, inplace=True)
p = binarizer.inverse_transform(y_pred)
df_p = pd.DataFrame({'Predducted':p})

yTest = binarizer.inverse_transform(y_test)
df_y = pd.DataFrame({'Tags':yTest})

df_p = pd.concat([X_test1, df_y, df_p], axis= 1)
df_p
# Actual Application
df_app = pd.DataFrame({
    'Title':['How to handle or avoid a stack overflow in C++',
             "Python getopt module error NameError: name 'opts' is not defined after importing",
             "get append element but got undefined",
             "I have a problem with pip",
             "Best way to center a <div> on a page vertically and horizontally? "],
    'Body':['In C++ a stack overflow usually leads to an unrecoverable crash of the program. For programs that need to be really robust, this is an unacceptable behaviour, particularly because stack size is limited. A few questions about how to handle the problem. Is there a way to prevent stack overflow by a general technique. (A scalable, robust solution, that includes dealing with external libraries eating a lot of stack, etc.) Is there a way to handle stack overflows in case they occur? Preferably, the stack gets unwound until theres a handler to deal with that kinda issue. There are languages out there, that have threads with expandable stacks. Is something like that possible in C++? Any other helpful comments on the solution of the C++ behaviour would be appreciated.',
            "I'm trying to take in two arguments from the console. The following code seems to have worked on my colleague's computer, so I'm not sure why it is giving me an error when trying to run it on mine. I am on a Mac.import getoptimport sysquestion_id= Nonearg_student = Noneargv = sys.argv[1:]print('test')try:opts, args = getopt.getopt(argv, 'i:s:', ['question_id=','arg_student='])except:print('Error')for opt, arg in opts:if opt in ['-i', '--question_id']:question_id = argelif opt in ['-s', '--arg_student']:arg_student = argprint('Question Number: ' + question_id)print('Student response: ' + arg_student)This is the error I am getting:ErrorTraceback (most recent call last):File '/Users/ailanysmacbook/github/AutomatedEssayGrading/AutomatedEssayGrading/input.py', line 1, in <module>import getoptFile '/Users/ailanysmacbook/github/AutomatedEssayGrading/AutomatedEssayGrading/getopt.py', line 20, in <module>for opt, arg in opts:NameError: name 'opts' is not definedIt seems to be happening right after I try importing it. Do I need to install something? I'm not sure what's missing.",
            "I have append element on my page, and after the element was append i want to send the element value with ajax in javascrypt. But i got response undefinedappend element jquery",
            "I am facimg problems in installing packages from pip",
            "Best way to center a <div> element on a page both vertically and horizontally?I know that margin-left: auto; margin-right: auto; will center on the horizontal, but what is the best way to do it vertically, too?"]
    })
    # preprocessing title and body
df_app['Title'] = df_app['Title'].apply(lambda x: remove_html(x))
df_app['Title'] = df_app['Title'].apply(lambda x: remove_stopwords(x))
df_app['Title'] = df_app['Title'].apply(lambda x: remove_punc(x))
df_app['Title'] = df_app['Title'].apply(lambda x: stem_text(x))

df_app['Body'] = df_app['Body'].apply(lambda x: remove_html(x))
df_app['Body'] = df_app['Body'].apply(lambda x: remove_stopwords(x))
df_app['Body'] = df_app['Body'].apply(lambda x: remove_punc(x))
df_app['Body'] = df_app['Body'].apply(lambda x: stem_text(x))

df_app
x_title = vectorizer_title.transform(df_app['Title'])
x_body = vectorizer_body.transform(df_app['Body'])

P_app = hstack([x_title, x_body])
P_app.shape

py= clf.predict(P_app)
binarizer.inverse_transform(py)
